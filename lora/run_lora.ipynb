{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/markmoussa/.pyenv/versions/mambaforge/envs/mlx/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from lora_module import run_lora_finetuning, run_lora_generate, run_lora_test\n",
    "from lora import load_pretrained_model, load_datasets, train, evaluate, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained model and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model args\n",
    "model_path = './models/Mistral-7B-Instruct-v0.2-4bit-mlx'\n",
    "lora_layers = 16\n",
    "seed = 0\n",
    "resume_adapter_file = None\n",
    "\n",
    "model_kwargs = {\n",
    "    'batch_size': 4,\n",
    "    'iters': 20,\n",
    "    'adapter_file': \"adapters.npz\",\n",
    "    'learning_rate': 1e-5,\n",
    "    'val_batches': 25,\n",
    "    'steps_per_report': 10,\n",
    "    'steps_per_eval': 200,\n",
    "}\n",
    "\n",
    "data_path =  './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 1244.041M\n",
      "Trainable parameters 1.704M\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, config = load_pretrained_model(model_path, lora_layers, seed, resume_adapter_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = load_datasets(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set length: 3348\n",
      "train_set sample: <s>[INST] Assist a non-verbal autistic individual in communicating their thoughts or needs through selected images. Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity. Be empathetic and direct. Look for deeper meanings in the input. Keep the tone practical and straight forward. Q: What is a good sentence a user might want to say given the following words: stars, fascinating?[/INST] A: Stars fascinate me.</s>\n",
      "valid_set length: 418\n",
      "test_set length: 419\n"
     ]
    }
   ],
   "source": [
    "print(f'train_set length: {len(train_set)}')\n",
    "print(f'train_set sample: {train_set[0]}')\n",
    "print(f'valid_set length: {len(valid_set)}')\n",
    "print(f'test_set length: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_lora_finetuning(model_path, data_path, lora_layers, batch_size, iters, seed, resume_adapter_file, adapter_file, learning_rate, val_batches, steps_per_report, steps_per_eval)\n",
    "train(model, train_set, valid_set, tokenizer, **model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test args\n",
    "model_path = './models/Mistral-7B-Instruct-v0.2-4bit-mlx'\n",
    "data_path = './data/'\n",
    "adapter_file = \"adapters.npz\"\n",
    "test_batches = 100\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 5.043103066025578\n"
     ]
    }
   ],
   "source": [
    "# run_lora_test(model_path, data_path, adapter_file, test_batches, batch_size)\n",
    "test_loss = evaluate(model, test_set, tokenizer, batch_size, test_batches)\n",
    "print(f\"Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate args\n",
    "model_path = './models/Mistral-7B-Instruct-v0.2-4bit-mlx'\n",
    "num_tokens = 100\n",
    "temp = 0.8\n",
    "adapter_file = \"adapters.npz\"\n",
    "\n",
    "prompt = \"\"\"Assist a non-verbal autistic individual in communicating their thoughts or needs through selected images.\n",
    "Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity.\n",
    "- Be empathetic and direct.\n",
    "- Look for deeper meanings in the input.\n",
    "- Keep the tone practical and straightforward.\n",
    "Only give the output for the input provided. Do not come up with new inputs after.\n",
    "input: chicken, soup, dinner\n",
    "output: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assist a non-verbal autistic individual in communicating their thoughts or needs through selected images.\n",
      "Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity.\n",
      "- Be empathetic and direct.\n",
      "- Look for deeper meanings in the input.\n",
      "- Keep the tone practical and straightforward.\n",
      "Only give the output for the input provided. Do not come up with new inputs after.\n",
      "input: chicken, soup, dinner\n",
      "output: \n",
      "I see a chicken and soup. It looks like we're having dinner tonight.\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate(model, prompt, tokenizer, num_tokens, temp, adapter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with raw prompt\n",
    "Generating using prompt as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate args\n",
    "# model_path = './models/Mistral-7B-Instruct-v0.2'\n",
    "model_path = './models/OpenHermes-2.5-Mistral-7B/'\n",
    "num_tokens = 100\n",
    "temp = 0.1\n",
    "adapter_file = \"adapters.npz\"\n",
    "\n",
    "prompt = \"\"\"<s> [INST] Assist a non-verbal autistic individual in communicating their thoughts or needs through selected images.\n",
    " Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity.\\\n",
    " Be empathetic and direct.\\\n",
    " Look for deeper meanings in the input.\\\n",
    " Keep the tone practical and straightforward.\\\n",
    " Only give the output for the input provided. Do not come up with new inputs after.\\\n",
    " Write the output in first-person.\\\n",
    " Write the output in simple, direct language. Use 2 sentences or less. Only give the answer, no explanation.\\\n",
    " Only give one output. Predict the STOP EOS token called </s> after your output.\\\n",
    " input: love, slippers, outside [/INST]\n",
    " output: </s>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with preprocessed prompt\n",
    "Preprocesses the prompt with ChatML format before generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "# just adding ChatML chat template here manually because it's SO annoying how this whole model thing is structured\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_chat_ml(sample):\n",
    "\n",
    "    # for Mistral 7B Instruct v0.2 specifically, because apparently chat template has no \"system\" part\n",
    "    # prompt = [\n",
    "    #     {\"role\": \"user\", \"content\": f\"Assist a non-verbal autistic individual in communicating their thoughts or needs through selected images. Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity. Be empathetic and direct. Look for deeper meanings in the input. Keep the tone practical and straight forward. \" + input_str},\n",
    "    #     {\"role\": \"assistant\", \"content\": output_str}\n",
    "    # ]\n",
    "\n",
    "    # for OpenHermes 2.5 Mistral 7B specifically\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": f\"Assist a non-verbal autistic individual in communicating their thoughts or needs through selected images. Your task: infer and articulate the message in first-person, using simple, direct language with empathy and clarity. Be empathetic and direct. Look for deeper meanings in the input. Keep the tone practical and straight forward.\"},\n",
    "        {\"role\": \"user\", \"content\": sample}\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(prompt, tokenize=False, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "\n",
    "formatted_prompt = preprocess_chat_ml(\"love, slippers, outside\")\n",
    "print(formatted_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
